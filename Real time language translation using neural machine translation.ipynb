{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28383ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, LSTM, Embedding, Concatenate\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of the vocabulary.\n",
    "# the bigger the vocabulary the better the translation but the longer the model will take to train.\n",
    "VOCAB_SIZE = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('/kaggle/input/europarl-parallel-corpus-19962011/english_french.csv')\n",
    "\n",
    "dataframe = dataframe.dropna()\n",
    "\n",
    "\n",
    "dataframe['English'] = dataframe['English'].map(lambda x: 'ssss ' + str(x) + ' eeee')\n",
    "\n",
    "english_tokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "english_tokenizer.fit_on_texts(dataframe['English'])\n",
    "\n",
    "dataframe['English_sequences'] = english_tokenizer.texts_to_sequences(dataframe['English'])\n",
    "\n",
    "dataframe['French'] = dataframe['French'].astype('str')\n",
    "\n",
    "french_tokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "french_tokenizer.fit_on_texts(dataframe['French'])\n",
    "\n",
    "dataframe['French_sequences'] = french_tokenizer.texts_to_sequences(dataframe['French'])\n",
    "#dataframe['French_sequences'] = dataframe['French_sequences'].map(lambda x: x[::-1])  #reversed sequences\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0711ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokeniser dictionary does not assign a word to the key'0'  just added a placeholder word\n",
    "# as when I was originally testing and making errors the model would not throw an error. \n",
    "english_tokenizer.index_word[0] = '<****>'\n",
    "french_tokenizer.index_word[0] = '<****>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fec150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_generator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, input_sequences, output_sequences, batch_size=128, shuffle=True):\n",
    "        self.input_sequences = input_sequences\n",
    "        self.output_sequences = output_sequences\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(input_sequences))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)//self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        \n",
    "        input_seq = self.__get_input_sequences(batch_indexes)\n",
    "        output_seq = self.__get_output_sequences(batch_indexes)\n",
    "\n",
    "        output_seq_in = output_seq[:,:-1]\n",
    "        output_seq_out = np.expand_dims(output_seq[:,1:], axis=-1)\n",
    "        \n",
    "        return [input_seq, output_seq_in], output_seq_out    \n",
    "        \n",
    "        \n",
    "    def __get_input_sequences(self, indexes):\n",
    "        input_seq = self.input_sequences[indexes]\n",
    "        input_seq = sequence.pad_sequences(input_seq, maxlen=50, padding='pre', truncating='pre')\n",
    "        return input_seq\n",
    "    \n",
    "    def __get_output_sequences(self, indexes):\n",
    "        output_seq = self.output_sequences[indexes]\n",
    "        output_seq = sequence.pad_sequences(output_seq, maxlen=50, padding='post', truncating='post')\n",
    "        return output_seq\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "\n",
    "train, test = train_test_split(dataframe, test_size=0.05, random_state=42)\n",
    "\n",
    "train_generator = Data_generator(train['French_sequences'].values, train['English_sequences'].values)\n",
    "test_generator = Data_generator(test['French_sequences'].values, test['English_sequences'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d38cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128  # size of the word embedding\n",
    "LATENT_SIZE = 256 # size of the LSTM latent size\n",
    "\n",
    "# these sizes can be played around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is quite compliciated\n",
    "    \n",
    "    \n",
    "class Encoder_layer(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, latent_size, **kwargs):\n",
    "        super(Encoder_layer, self).__init__(**kwargs)\n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_size)\n",
    "        self.alignment_lstm = LSTM(latent_size//2, return_sequences=True, return_state=True) # concentrating on alignment/attention  tried half size\n",
    "        self.encoder_lstm =  LSTM(latent_size, return_sequences=True, return_state=True)  # concnetrating on decoding the sentence\n",
    "        \n",
    "           \n",
    "    def call(self, inputs):\n",
    "        embedding_out = self.embedding_layer(inputs)\n",
    "        alignment_seq, alignment_h, alignment_c = self.alignment_lstm(embedding_out)\n",
    "        encoder_seq, encoder_h, encoder_c = self.encoder_lstm(embedding_out)\n",
    "        \n",
    "        return [alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c]\n",
    "    \n",
    "    \n",
    "class Decoder_layer(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, latent_size, **kwargs):\n",
    "        super(Decoder_layer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.embedding_layer = Embedding(vocab_size, embedding_size)\n",
    "        self.alignment_lstm = LSTM(latent_size//2, return_sequences=True, return_state=True)\n",
    "        self.decoder_lstm =  LSTM(latent_size, return_sequences=True, return_state=True)\n",
    "        \n",
    "        #self.context_layer = Attention_layer(latent_size)\n",
    "        self.concat_layer = Concatenate()\n",
    "        self.dense_layer = Dense(vocab_size, activation='softmax')  \n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        lang_in, encoder_alignment_seq, encoder_alignment_h, encoder_alignment_c, encoder_out, encoder_h, encoder_c = inputs\n",
    "        \n",
    "        embedding_out = self.embedding_layer(lang_in)\n",
    "        alignment_seq, alignment_h, alignment_c = self.alignment_lstm(embedding_out, initial_state=[encoder_alignment_h, encoder_alignment_c])\n",
    "        decoder_seq, decoder_h, decoder_c = self.decoder_lstm(embedding_out, initial_state=[encoder_h, encoder_c])\n",
    "        \n",
    "        scores = tf.matmul(alignment_seq, encoder_alignment_seq, transpose_b=True)\n",
    "        alignment = tf.nn.softmax(scores, axis=-1)\n",
    "        context = tf.matmul(alignment, encoder_out)\n",
    "        concat = self.concat_layer([decoder_seq, context])\n",
    "        prediction = self.dense_layer(concat)\n",
    "        \n",
    "        return [prediction, alignment_h, alignment_c, decoder_h, decoder_c]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafaaff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complicated becuase I wanted to split up training from the inference, otherwise the entire network has to be run to predict each individual word. \n",
    "# this way the states are remembered so only the final portion need be run to predict each word.\n",
    "\n",
    "\n",
    "original_lang_input = Input(shape=(None,))\n",
    "translated_lang_input = Input(shape=(None,))\n",
    "\n",
    "alignment_h_input = Input(shape=(LATENT_SIZE//2,))\n",
    "alignment_c_input = Input(shape=(LATENT_SIZE//2,))\n",
    "decoder_h_input = Input(shape=(LATENT_SIZE,))\n",
    "decoder_c_input = Input(shape=(LATENT_SIZE,))\n",
    "\n",
    "alignment_input = Input(shape=(None, LATENT_SIZE//2))\n",
    "decoder_input = Input(shape=(None, LATENT_SIZE))\n",
    "\n",
    "# layers \n",
    "\n",
    "encoder = Encoder_layer(VOCAB_SIZE, EMBEDDING_SIZE, LATENT_SIZE)\n",
    "decoder = Decoder_layer(VOCAB_SIZE, EMBEDDING_SIZE, LATENT_SIZE)\n",
    "\n",
    "#connecting the encoder_model\n",
    "alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c = encoder(original_lang_input)\n",
    "\n",
    "encoder_model = Model([original_lang_input], [alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c], name='encoder')\n",
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the decoder_model\n",
    "\n",
    "decoder_prediction, decoder_alignment_h, decoder_alignment_c, decoder_h, decoder_c = decoder([translated_lang_input, \n",
    "                                                                              alignment_input,\n",
    "                                                                              alignment_h_input, \n",
    "                                                                              alignment_c_input, \n",
    "                                                                              decoder_input, \n",
    "                                                                              decoder_h_input, \n",
    "                                                                              decoder_c_input])\n",
    "\n",
    "decoder_model = Model(inputs=[translated_lang_input, alignment_input,alignment_h_input, alignment_c_input, decoder_input,decoder_h_input, decoder_c_input],\n",
    "                     outputs=[decoder_prediction, decoder_alignment_h, decoder_alignment_c, decoder_h, decoder_c], name='decoder')\n",
    "\n",
    "decoder_model.summary()\n",
    "\n",
    "training_pred,_,_,_,_ = decoder_model([translated_lang_input, alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c])\n",
    "training_model = Model(inputs=[original_lang_input, translated_lang_input], outputs=[training_pred])\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "training_model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')\n",
    "history = training_model.fit_generator(train_generator, epochs=5, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    plt.plot(history.history['loss'], c='r')\n",
    "    plt.plot(history.history['val_loss'], c ='g')\n",
    "    plt.show()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(dataframe, index):\n",
    "    print('Original french: ' + dataframe['French'].iloc[index])\n",
    "    print()\n",
    "    print('Original english: '+ dataframe['English'].iloc[index])\n",
    "    print()\n",
    "    \n",
    "    french_input = dataframe['French_sequences'].iloc[index]\n",
    "    word = 'ssss'\n",
    "    token = english_tokenizer.word_index[word]\n",
    "    sentence = [word]\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c = encoder_model.predict(np.expand_dims(french_input,axis=0))\n",
    "    while word != 'eeee' and count < 50:\n",
    "        decoder_pred, alignment_h, alignment_c, encoder_h, encoder_c = decoder_model.predict([np.expand_dims(token, axis=0), alignment_seq, alignment_h, alignment_c, encoder_seq, encoder_h, encoder_c])\n",
    "        \n",
    "        token = np.argmax(decoder_pred)\n",
    "        word = english_tokenizer.index_word[token]\n",
    "        sentence.append(word)\n",
    "        count += 1    \n",
    "    print('Predicted english: ' + ' '.join(sentence[1:-1]))   \n",
    "    print()\n",
    "    print('-------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b021f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some predictions on the test data, The model has not been traind on this.\n",
    "\n",
    "for i in range(50):\n",
    "    try:\n",
    "        predict_sentence(test,i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the predictions on the training data, the model has been trained on this, therfore expect these to be better? \n",
    "for i in range(30):\n",
    "    try:\n",
    "        predict_sentence(train,i)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
